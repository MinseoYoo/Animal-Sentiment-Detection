{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":840806,"sourceType":"datasetVersion","datasetId":59760},{"sourceId":6665954,"sourceType":"datasetVersion","datasetId":3546787},{"sourceId":9155124,"sourceType":"datasetVersion","datasetId":5530529}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics\n!pip install -U ipywidgets\n!pip install torch torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-12T10:58:35.835086Z","iopub.execute_input":"2024-08-12T10:58:35.835454Z","iopub.status.idle":"2024-08-12T10:59:16.172117Z","shell.execute_reply.started":"2024-08-12T10:58:35.835425Z","shell.execute_reply":"2024-08-12T10:59:16.171133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pandas as pd\n\n# Check if a GPU is available and move the model to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T10:59:16.173951Z","iopub.execute_input":"2024-08-12T10:59:16.174268Z","iopub.status.idle":"2024-08-12T10:59:20.755461Z","shell.execute_reply.started":"2024-08-12T10:59:16.174237Z","shell.execute_reply":"2024-08-12T10:59:20.754466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# YOLO 알고리즘을 구현한 모델 다운로드\n# COCO 데이터셋에서 학습된 YOLOv3 모델 사용\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# 이미지를 불러오고 전처리 수행\n# PyTorch의 TorchVision 라이브러리를 사용하여 이미지를 불러오고, 이미지 크기를 조정하고, 채널 순서를 변경하여 모델의 입력 형식에 맞게 전처리를 수행\nimage = Image.open('/kaggle/input/pets-facial-expression-dataset/Master Folder/train/Angry/02.jpg')\ntransform = transforms.Compose([\n    transforms.Resize((800, 800)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\nimage = transform(image)\n\n# 전처리된 이미지를 모델에 입력으로 넣고 객체 검출을 수행\nmodel.eval()\nwith torch.no_grad():\n    predictions = model([image])\n\n# 예측된 bounding box들과 해당 객체의 클래스\nbboxes = predictions[0]['boxes']\nlabels = predictions[0]['labels']\n\n# 원본 이미지 위에 bounding box와 클래스 이름을 표시할 수 있습니다.\nfig, ax = plt.subplots(1)\nax.imshow(image.permute(1, 2, 0))\n\nfor bbox, label in zip(bboxes, labels):\n    ax.add_patch(plt.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], fill=False, edgecolor='red', linewidth=2))\n    ax.text(bbox[0], bbox[1], f'{label.item()}', color='red', fontsize=12)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-12T11:02:04.436450Z","iopub.execute_input":"2024-08-12T11:02:04.436790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom ultralytics import YOLO\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# YOLOv8 모델 로드\nmodel = YOLO('yolov8n.pt')  # Replace 'yolov8n.pt' with the appropriate model checkpoint\n\n# 클래스 이름 가져오기\nclass_names = model.names\n\n# 이미지 파일이 있는 디렉토리 경로 설정\nimage_dir = '/kaggle/input/pets-facial-expression-dataset/Master Folder/train/Angry/'  # 여기에 올바른 경로를 입력합니다\noutput_dir = 'annotated_images/'  # Annotated images will be saved here\n\n# Annotated images 디렉토리가 존재하지 않으면 생성\nos.makedirs(output_dir, exist_ok=True)\n\n# 디렉토리 내의 모든 이미지 파일에 대해 반복\nfor filename in os.listdir(image_dir):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 이미지 파일 형식 필터링\n        image_path = os.path.join(image_dir, filename)\n        image = Image.open(image_path)\n        image_np = np.array(image)\n\n        # YOLO 모델에 이미지를 입력으로 넣고 객체 검출 수행\n        results = model(image_np)\n\n        # 예측된 bounding box들과 해당 객체의 클래스\n        pred = results[0]\n\n        # 각 예측의 bounding box와 클래스 정보를 추출\n        bboxes = pred.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]\n        confidences = pred.boxes.conf.cpu().numpy()  # confidence scores\n        classes = pred.boxes.cls.cpu().numpy()  # class ids\n\n        # 원본 이미지 위에 bounding box와 클래스 이름을 표시\n        fig, ax = plt.subplots(1)\n        ax.imshow(image)\n\n        for bbox, conf, cls in zip(bboxes, confidences, classes):\n            x1, y1, x2, y2 = bbox\n            class_name = class_names[int(cls)]\n            ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2))\n            ax.text(x1, y1, f'{class_name} {conf:.2f}', color='red', fontsize=12)\n\n        # Annotated image 저장\n        output_path = os.path.join(output_dir, filename)\n        plt.savefig(output_path)\n        plt.close()\n        \n        print(f'Processed and saved: {output_path}')\n\nprint('All images have been processed and saved.')","metadata":{"execution":{"iopub.execute_input":"2024-08-12T11:02:29.302991Z","iopub.status.idle":"2024-08-12T11:03:12.662967Z","shell.execute_reply.started":"2024-08-12T11:02:29.302952Z","shell.execute_reply":"2024-08-12T11:03:12.661677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom ultralytics import YOLO\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# YOLOv8 모델 로드\nmodel = YOLO('yolov8n.pt')  # Replace 'yolov8n.pt' with the appropriate model checkpoint\n\n# 클래스 이름 가져오기\nclass_names = model.names\n\n# 이미지 파일이 있는 디렉토리 경로 설정\nimage_dir = '/kaggle/input/pets-facial-expression-dataset/Master Folder/train/Angry/'  # 여기에 올바른 경로를 입력합니다\noutput_dir = 'annotated_images/'  # Annotated images will be saved here\n\n# Annotated images 디렉토리가 존재하지 않으면 생성\nos.makedirs(output_dir, exist_ok=True)\n\n# 디렉토리 내의 모든 이미지 파일에 대해 반복\nfor filename in os.listdir(image_dir):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 이미지 파일 형식 필터링\n        image_path = os.path.join(image_dir, filename)\n        image = Image.open(image_path)\n        image_np = np.array(image)\n\n        # YOLO 모델에 이미지를 입력으로 넣고 객체 검출 수행\n        results = model(image_np)\n\n        # 예측된 bounding box들과 해당 객체의 클래스\n        pred = results[0]\n\n        # 각 예측의 bounding box와 클래스 정보를 추출\n        bboxes = pred.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]\n        confidences = pred.boxes.conf.cpu().numpy()  # confidence scores\n        classes = pred.boxes.cls.cpu().numpy()  # class ids\n\n        # 원본 이미지 위에 bounding box와 클래스 이름을 표시\n        fig, ax = plt.subplots(1)\n        ax.imshow(image)\n\n        for bbox, conf, cls in zip(bboxes, confidences, classes):\n            x1, y1, x2, y2 = bbox\n            class_name = class_names[int(cls)]\n            ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2))\n            ax.text(x1, y1, f'{class_name} {conf:.2f}', color='red', fontsize=12)\n\n        # Annotated image 저장\n        output_path = os.path.join(output_dir, filename)\n        plt.savefig(output_path)\n        plt.close()\n        \n        # Display the annotated image\n        annotated_image = Image.open(output_path)\n        plt.figure(figsize=(8, 8))\n        plt.imshow(annotated_image)\n        plt.axis('off')\n        plt.title(f\"Annotated Image: {filename}\")\n        plt.show()\n\n        print(f'Processed and saved: {output_path}')\n\nprint('All images have been processed and saved.')","metadata":{"execution":{"iopub.status.busy":"2024-08-12T11:03:12.665901Z","iopub.execute_input":"2024-08-12T11:03:12.666458Z","iopub.status.idle":"2024-08-12T11:05:03.142858Z","shell.execute_reply.started":"2024-08-12T11:03:12.666421Z","shell.execute_reply":"2024-08-12T11:05:03.141870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom ultralytics import YOLO\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# YOLOv8 모델 로드\nmodel = YOLO('yolov8n.pt')  # you can replace 'yolov8n.pt' with the appropriate model checkpoint\n\n# 클래스 이름 가져오기\nclass_names = model.names\n\n# 이미지를 불러오고 전처리 수행\nimage_path = '/kaggle/input/animals10/raw-img/cane/OIF-e2bexWrojgtQnAPPcUfOWQ.jpeg'  # 여기에 올바른 경로를 입력합니다\nimage = Image.open(image_path)\nimage = np.array(image)\n\n# YOLO 모델에 이미지를 입력으로 넣고 객체 검출 수행\nresults = model(image)\n\n# 예측된 bounding box들과 해당 객체의 클래스\n# results[0]에는 예측된 결과가 포함됩니다.\npred = results[0]\n\n# 각 예측의 bounding box와 클래스 정보를 추출\nbboxes = pred.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]\nconfidences = pred.boxes.conf.cpu().numpy()  # confidence scores\nclasses = pred.boxes.cls.cpu().numpy()  # class ids\n\n# 원본 이미지 위에 bounding box와 클래스 이름을 표시\nfig, ax = plt.subplots(1)\nax.imshow(image)\n\nfor bbox, conf, cls in zip(bboxes, confidences, classes):\n    x1, y1, x2, y2 = bbox\n    class_name = class_names[int(cls)]\n    ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2))\n    ax.text(x1, y1, f'{class_name} {conf:.2f}', color='red', fontsize=12)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-12T11:05:03.144131Z","iopub.execute_input":"2024-08-12T11:05:03.144533Z","iopub.status.idle":"2024-08-12T11:05:03.765990Z","shell.execute_reply.started":"2024-08-12T11:05:03.144456Z","shell.execute_reply":"2024-08-12T11:05:03.765091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터셋 경로\ndata_yaml_content = \"\"\"\ntrain: /kaggle/input/cat-dataset/AugmnetedCatsData.v1i.yolov8/AugmnetedCatsData.v1i.yolov8/train/images\nval: /kaggle/input/cat-dataset/AugmnetedCatsData.v1i.yolov8/AugmnetedCatsData.v1i.yolov8/valid/images\ntest: /kaggle/input/cat-dataset/AugmnetedCatsData.v1i.yolov8/AugmnetedCatsData.v1i.yolov8/test/images\n\nnc: 1  # 클래스 수 (고양이 얼굴만 탐지할 경우 1로 설정)\nnames: ['cat_face']  # 클래스 이름\n\"\"\"\n\n# YAML 파일 생성\nyaml_file_path = '/kaggle/working/cat_data.yaml'\nwith open(yaml_file_path, 'w') as f:\n    f.write(data_yaml_content)\n\nprint(f\"YAML file created at: {yaml_file_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-12T11:05:03.767348Z","iopub.execute_input":"2024-08-12T11:05:03.767726Z","iopub.status.idle":"2024-08-12T11:05:03.775156Z","shell.execute_reply.started":"2024-08-12T11:05:03.767695Z","shell.execute_reply":"2024-08-12T11:05:03.774213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom ultralytics import YOLO\n\n# W&B 비활성화\nos.environ['WANDB_DISABLED'] = 'true'\n\n# YOLOv8 모델 로드\nmodel = YOLO('yolov8n.pt')\n\n# 데이터셋 경로 설정\ndata_yaml_path = '/kaggle/working/cat_data.yaml'\n\n# 모델 학습\nresults = model.train(data=data_yaml_path, epochs=10, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T11:05:03.776568Z","iopub.execute_input":"2024-08-12T11:05:03.777376Z","iopub.status.idle":"2024-08-12T11:07:26.394353Z","shell.execute_reply.started":"2024-08-12T11:05:03.777348Z","shell.execute_reply":"2024-08-12T11:07:26.393212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**학습 로그**\n\nEpoch: 현재 학습 중인 에폭(epoch) 번호입니다. 예를 들어, 1/10은 10 에폭 중 첫 번째 에폭을 의미합니다.\n\nGPU_mem: GPU 메모리 사용량입니다. 각 에폭 동안 사용된 GPU 메모리 양을 나타냅니다.\n\nbox_loss: 박스 회귀 손실(Box regression loss)입니다. 모델이 예측한 박스와 실제 박스 간의 차이를 측정합니다. 값이 낮을수록 좋습니다.\n\ncls_loss: 클래스 분류 손실(Classification loss)입니다. 모델이 객체의 클래스를 예측하는 데 발생하는 손실입니다. 값이 낮을수록 좋습니다.\n\ndfl_loss: 디스트리뷰티드 폴리곤 손실(Distributed Focal Loss)입니다. 객체 검출의 정확도를 향상시키기 위한 손실입니다.\n\nInstances: 한 배치에서 처리된 이미지의 수입니다.\n\nSize: 입력 이미지의 크기입니다.\n\n--------------\n\n**검증 결과**\n\nBox(P): 박스 예측의 정밀도(Precision)입니다. 모델이 예측한 박스가 실제 박스와 얼마나 일치하는지를 측정합니다.\n\nBox(R): 박스 예측의 재현율(Recall)입니다. 실제 박스를 얼마나 잘 찾아냈는지를 측정합니다.\n\nmAP50: IoU(Intersection over Union) 임계값 0.5에서의 평균 정밀도(Mean Average Precision)입니다. 높은 값일수록 좋습니다.\n\nmAP50-95: IoU 임계값 0.5부터 0.95까지의 평균 정밀도입니다. 다양한 IoU 임계값에서 모델의 성능을 측정합니다.\n\n--------------\n\n**학습 결과 요약**\n\nlr/pg0, lr/pg1, lr/pg2: 학습률(Learning Rate) 그래프입니다. 학습률의 변화를 시각화합니다.\n\nmetrics/mAP50(B): IoU 0.5에서의 평균 정밀도입니다. 0.995로 매우 높은 성능을 보여줍니다.\n\nmetrics/mAP50-95(B): IoU 0.5부터 0.95까지의 평균 정밀도입니다. 0.79로, 모델이 다양한 IoU 임계값에서 잘 동작함을 의미합니다.\n\nmetrics/precision(B): 객체 검출의 정밀도입니다. 0.9995로 매우 높은 정밀도를 보입니다.\n\nmetrics/recall(B): 객체 검출의 재현율입니다. 1.0으로, 모든 실제 객체를 성공적으로 검출했음을 의미합니다.\n\nmodel/GFLOPs: 모델의 계산 복잡도를 나타내는 GFLOPs(Giga Floating Point Operations)입니다. 8.194 GFLOPs입니다.\n\nmodel/parameters: 모델의 총 파라미터 수입니다. 3,010,043개입니다.\n\nmodel/speed_PyTorch(ms): PyTorch에서의 모델 추론 속도입니다. 이미지당 5.324ms입니다.\n\ntrain/box_loss, train/cls_loss, train/dfl_loss: 학습 데이터에 대한 손실 값입니다. 각 손실이 감소하는 것이 좋습니다.\n\nval/box_loss, val/cls_loss, val/dfl_loss: 검증 데이터에 대한 손실 값입니다. 학습 손실과 검증 손실 간의 차이를 비교하여 모델의 일반화 능력을 평가할 수 있습니다.\n\n결론\n성능: 모델은 높은 정밀도(Precision)와 재현율(Recall)을 기록하며, mAP50과 mAP50-95가 매우 높아 객체 검출 성능이 우수함을 나타냅니다.\n손실 감소: 학습 과정 동안 손실 값이 점차 감소하여 모델이 점점 더 좋은 성능을 보임을 의미합니다.\n속도: 모델의 추론 속도가 적절하며, 파라미터 수가 상당하여 복잡한 모델임을 알 수 있습니다.","metadata":{}},{"cell_type":"code","source":"test_path = '/kaggle/input/cat-dataset/AugmnetedCatsData.v1i.yolov8/AugmnetedCatsData.v1i.yolov8/test/images'\n\n# 테스트 데이터에 대해 모델 예측 수행\ntest_results = model.predict(source=test_path, save=True)\n\n# 결과 저장\nmodel.save('yolov8_cat_detection_model.pt')  # 학습된 모델 저장","metadata":{"execution":{"iopub.status.busy":"2024-08-12T11:07:26.396030Z","iopub.execute_input":"2024-08-12T11:07:26.396372Z","iopub.status.idle":"2024-08-12T11:07:28.452821Z","shell.execute_reply.started":"2024-08-12T11:07:26.396332Z","shell.execute_reply":"2024-08-12T11:07:28.451791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport os\nfrom ultralytics import YOLO\n\n# Load the trained YOLOv8 model\nmodel = YOLO('yolov8_cat_detection_model.pt')\n\n# Path to the test image directory\ntest_image_dir = '/kaggle/input/cat-dataset/AugmnetedCatsData.v1i.yolov8/AugmnetedCatsData.v1i.yolov8/test/images'\n\n# List files in the directory\nimage_files = os.listdir(test_image_dir)\nprint(f\"Images in directory: {image_files}\")\n\n# 이미지 디렉토리 내 모든 이미지를 처리합니다.\nfor image_file in image_files:\n    # 이미지 파일의 전체 경로\n    image_path = os.path.join(test_image_dir, image_file)\n    \n    # 이미지 로드 및 RGB로 변환\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Failed to load image: {image_file}\")\n        continue\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # 이미지에 대한 예측 결과 가져오기\n    results = model.predict(source=image_path)\n\n    # 결과 리스트에서 현재 이미지에 대한 결과 가져오기\n    result = results[0]\n\n    # Bounding boxes, class labels, 그리고 confidence scores를 직접 가져오기\n    boxes = result.boxes  # Boxes object\n    if boxes is not None:\n        for box in boxes:\n            # x, y 좌표, 너비, 높이, confidence score, 클래스 ID를 추출합니다.\n            x1, y1, x2, y2 = box.xyxy[0]  # Bounding box 좌표\n            conf = box.conf[0]  # Confidence score\n            cls = int(box.cls[0])  # Class ID\n            label = f\"{result.names[cls]} {conf:.2f}\"\n\n            # Bounding box 그리기\n            cv2.rectangle(image_rgb, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n            # 레이블 텍스트 추가\n            cv2.putText(image_rgb, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # 이미지 출력 (각 이미지별로 처리 결과를 확인하기 위해)\n    plt.figure(figsize=(8, 8))\n    plt.imshow(image_rgb)\n    plt.axis('off')\n    plt.title(f\"Predictions for {os.path.basename(image_path)}\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-12T11:07:28.454125Z","iopub.execute_input":"2024-08-12T11:07:28.454439Z","iopub.status.idle":"2024-08-12T11:07:59.511604Z","shell.execute_reply.started":"2024-08-12T11:07:28.454412Z","shell.execute_reply":"2024-08-12T11:07:59.510656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Load the trained YOLOv8 model\nmodel = YOLO('yolov8_cat_detection_model.pt')\n\n# 정확도 평가 (AP, Precision, Recall 등)\nmetrics = model.val(data=data_yaml_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T11:07:59.515110Z","iopub.execute_input":"2024-08-12T11:07:59.515435Z","iopub.status.idle":"2024-08-12T11:08:06.646674Z","shell.execute_reply.started":"2024-08-12T11:07:59.515408Z","shell.execute_reply":"2024-08-12T11:08:06.645587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Path to the image file\nimage_path = '/kaggle/working/runs/detect/train/F1_curve.png'\n\n# Load and display the image\nimg = mpimg.imread(image_path)\nplt.figure(figsize=(10, 6))\nplt.imshow(img)\nplt.axis('off')  # Hide axes\nplt.title('F1 Curve')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-12T11:08:57.820536Z","iopub.execute_input":"2024-08-12T11:08:57.821226Z","iopub.status.idle":"2024-08-12T11:08:58.641055Z","shell.execute_reply.started":"2024-08-12T11:08:57.821191Z","shell.execute_reply":"2024-08-12T11:08:58.639916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom ultralytics import YOLO\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# YOLOv8 모델 로드\nmodel = YOLO('yolov8_cat_detection_model.pt')  # Replace 'yolov8n.pt' with the appropriate model checkpoint\n\n# 클래스 이름 가져오기\nclass_names = model.names\n\n# 이미지 파일이 있는 디렉토리 경로 설정\nimage_dir = '/kaggle/input/pets-facial-expression-dataset/Master Folder/train/Angry/'  # 여기에 올바른 경로를 입력합니다\noutput_dir = 'annotated_images/'  # Annotated images will be saved here\n\n# Annotated images 디렉토리가 존재하지 않으면 생성\nos.makedirs(output_dir, exist_ok=True)\n\n# 디렉토리 내의 모든 이미지 파일에 대해 반복\nfor filename in os.listdir(image_dir):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 이미지 파일 형식 필터링\n        image_path = os.path.join(image_dir, filename)\n        image = Image.open(image_path)\n        image_np = np.array(image)\n\n        # YOLO 모델에 이미지를 입력으로 넣고 객체 검출 수행\n        results = model(image_np)\n\n        # 예측된 bounding box들과 해당 객체의 클래스\n        pred = results[0]\n\n        # 각 예측의 bounding box와 클래스 정보를 추출\n        bboxes = pred.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]\n        confidences = pred.boxes.conf.cpu().numpy()  # confidence scores\n        classes = pred.boxes.cls.cpu().numpy()  # class ids\n\n        # 원본 이미지 위에 bounding box와 클래스 이름을 표시\n        fig, ax = plt.subplots(1)\n        ax.imshow(image)\n\n        for bbox, conf, cls in zip(bboxes, confidences, classes):\n            x1, y1, x2, y2 = bbox\n            class_name = class_names[int(cls)]\n            ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2))\n            ax.text(x1, y1, f'{class_name} {conf:.2f}', color='red', fontsize=12)\n\n        # Annotated image 저장\n        output_path = os.path.join(output_dir, filename)\n        plt.savefig(output_path)\n        plt.close()\n        \n        # Display the annotated image\n        annotated_image = Image.open(output_path)\n        plt.figure(figsize=(8, 8))\n        plt.imshow(annotated_image)\n        plt.axis('off')\n        plt.title(f\"Annotated Image: {filename}\")\n        plt.show()\n\n        print(f'Processed and saved: {output_path}')\n\nprint('All images have been processed and saved.')","metadata":{},"execution_count":null,"outputs":[]}]}