{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport random\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nfrom PIL import Image\nimport torchvision.transforms as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-06T02:47:09.838283Z","iopub.execute_input":"2024-08-06T02:47:09.838563Z","iopub.status.idle":"2024-08-06T02:47:20.606926Z","shell.execute_reply.started":"2024-08-06T02:47:09.838537Z","shell.execute_reply":"2024-08-06T02:47:20.606180Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.device(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:47:25.527895Z","iopub.execute_input":"2024-08-06T02:47:25.528670Z","iopub.status.idle":"2024-08-06T02:47:25.599524Z","shell.execute_reply.started":"2024-08-06T02:47:25.528634Z","shell.execute_reply":"2024-08-06T02:47:25.598610Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size= 64\nimg_size = 224","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:47:28.752797Z","iopub.execute_input":"2024-08-06T02:47:28.753123Z","iopub.status.idle":"2024-08-06T02:47:28.758145Z","shell.execute_reply.started":"2024-08-06T02:47:28.753098Z","shell.execute_reply":"2024-08-06T02:47:28.757177Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_dir = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\ntrain_images = '/kaggle/input/isic-2024-challenge/train-image/image'","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:47:33.368044Z","iopub.execute_input":"2024-08-06T02:47:33.368910Z","iopub.status.idle":"2024-08-06T02:47:33.373033Z","shell.execute_reply.started":"2024-08-06T02:47:33.368880Z","shell.execute_reply":"2024-08-06T02:47:33.372190Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(train_dir,usecols=['isic_id','target'])\nisic_id = df['isic_id'].tolist()\ntargets = df['target'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:47:34.288719Z","iopub.execute_input":"2024-08-06T02:47:34.289042Z","iopub.status.idle":"2024-08-06T02:47:40.054533Z","shell.execute_reply.started":"2024-08-06T02:47:34.289014Z","shell.execute_reply":"2024-08-06T02:47:40.053506Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_transforms(image_size):\n    transforms_train = transforms.Compose([\n        transforms.ToPILImage(),  # Add this to convert tensor to PIL Image for further processing\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.1, 0.2)),\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n    ])\n    \n    transforms_val = transforms.Compose([\n        transforms.ToPILImage(),  # Add this to convert tensor to PIL Image for further processing\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor()\n    ])\n    \n    return transforms_train, transforms_val","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:49:17.269180Z","iopub.execute_input":"2024-08-06T02:49:17.269849Z","iopub.status.idle":"2024-08-06T02:49:17.285404Z","shell.execute_reply.started":"2024-08-06T02:49:17.269817Z","shell.execute_reply":"2024-08-06T02:49:17.284375Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"img_size = 224\ntransform_train, transform_val = get_transforms(img_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:49:18.809934Z","iopub.execute_input":"2024-08-06T02:49:18.810375Z","iopub.status.idle":"2024-08-06T02:49:18.815081Z","shell.execute_reply.started":"2024-08-06T02:49:18.810345Z","shell.execute_reply":"2024-08-06T02:49:18.814156Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## 이미지 전처리\nSkinDataset 클래스 만들어 불러오고 train/valid/test 나누기","metadata":{}},{"cell_type":"code","source":"class SkinDataset(Dataset):\n    def __init__(self, isic_id, images, targets, transforms, device, split='train'):\n        self.transforms = transforms\n        self.isic_id = isic_id\n        self.targets = targets\n        self.device = device\n        self.images = images\n        \n        dataset_size = int(len(self.isic_id)*0.3)\n        indices = list(range(dataset_size))\n        random.shuffle(indices)\n        train_split = int(0.8 * dataset_size)\n        if split == 'train':\n            self.indices = indices[:train_split]\n        elif split == 'valid':\n            self.indices = indices[train_split:]\n        else:\n            raise ValueError(\"Split must be 'train' or 'valid'\")\n    \n    def open_image(self, images, key):\n        with h5py.File(images, 'r') as f:\n            image_data = f[key][()]\n            nparr = np.frombuffer(image_data, np.uint8)\n            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n        return img\n\n    def __len__(self):\n        return len(self.indices)\n    \n    def __preprocess(self, image):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n        clahe = cv2.createCLAHE(tileGridSize=(8,8), clipLimit=1)\n        image[:,:,0] = clahe.apply(image[:,:,0])\n        image = cv2.cvtColor(image, cv2.COLOR_LAB2RGB)\n        return image\n    \n    def __getitem__(self, index):\n        idx = self.indices[index]\n        key = self.isic_id[idx]\n        if len(self.images.split('.')) > 1:\n            image = self.open_image(self.images, key)\n        else:\n            image = cv2.imread(os.path.join(self.images, f'{key}.jpg'))\n        \n        image = self.__preprocess(image)\n        image = np.moveaxis(image, -1, 0)\n        image = torch.from_numpy(image)\n        \n        if self.transforms:\n            image = self.transforms(image)\n        \n        label = torch.tensor(self.targets[idx])\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:49:24.183898Z","iopub.execute_input":"2024-08-06T02:49:24.184262Z","iopub.status.idle":"2024-08-06T02:49:24.197921Z","shell.execute_reply.started":"2024-08-06T02:49:24.184230Z","shell.execute_reply":"2024-08-06T02:49:24.196938Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset = SkinDataset(isic_id, train_images, targets, transform_train, device='cuda', split='train')\nval_dataset = SkinDataset(isic_id, train_images, targets, transform_val, device='cuda', split='valid')","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:49:26.225126Z","iopub.execute_input":"2024-08-06T02:49:26.225488Z","iopub.status.idle":"2024-08-06T02:49:26.433187Z","shell.execute_reply.started":"2024-08-06T02:49:26.225462Z","shell.execute_reply":"2024-08-06T02:49:26.432441Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nvalid_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:49:27.461821Z","iopub.execute_input":"2024-08-06T02:49:27.462181Z","iopub.status.idle":"2024-08-06T02:49:27.467011Z","shell.execute_reply.started":"2024-08-06T02:49:27.462151Z","shell.execute_reply":"2024-08-06T02:49:27.466183Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## EfficientNet 사용","metadata":{}},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:35:45.248719Z","iopub.execute_input":"2024-08-05T10:35:45.249392Z","iopub.status.idle":"2024-08-05T10:36:02.504998Z","shell.execute_reply.started":"2024-08-05T10:35:45.249362Z","shell.execute_reply":"2024-08-05T10:36:02.503841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet\neff_model = EfficientNet.from_pretrained('efficientnet-b0')","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:36:02.507473Z","iopub.execute_input":"2024-08-05T10:36:02.508182Z","iopub.status.idle":"2024-08-05T10:36:03.196021Z","shell.execute_reply.started":"2024-08-05T10:36:02.508142Z","shell.execute_reply":"2024-08-05T10:36:03.194991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_evaluate(model, train_loader, valid_loader, num_epochs=5, device='cuda'):\n    model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        # Training loop\n        for images, labels in train_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n            # Calculate accuracy\n            _, predicted = torch.max(outputs, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        train_accuracy = 100 * correct_train / total_train\n        print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.3f}, Training Accuracy: {train_accuracy:.3f}%\")\n\n        # Validation loop\n        model.eval()\n        valid_loss = 0.0\n        correct_valid = 0\n        total_valid = 0\n        with torch.no_grad():\n            for images, labels in valid_loader:\n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n                valid_loss += loss.item()\n\n                # Calculate accuracy\n                _, predicted = torch.max(outputs, 1)\n                total_valid += labels.size(0)\n                correct_valid += (predicted == labels).sum().item()\n\n        avg_valid_loss = valid_loss / len(valid_loader)\n        valid_accuracy = 100 * correct_valid / total_valid\n        print(f\"Epoch {epoch+1}, Validation Loss: {avg_valid_loss:.3f}, Validation Accuracy: {valid_accuracy:.3f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T03:01:22.740459Z","iopub.execute_input":"2024-08-06T03:01:22.740826Z","iopub.status.idle":"2024-08-06T03:01:22.753200Z","shell.execute_reply.started":"2024-08-06T03:01:22.740798Z","shell.execute_reply":"2024-08-06T03:01:22.752247Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_and_evaluate(eff_model, train_loader, valid_loader, 5)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T11:25:05.448498Z","iopub.execute_input":"2024-08-05T11:25:05.448876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ViT 사용","metadata":{}},{"cell_type":"code","source":"vit_model = models.vision_transformer.vit_b_16(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T03:00:54.435254Z","iopub.execute_input":"2024-08-06T03:00:54.435649Z","iopub.status.idle":"2024-08-06T03:00:55.992927Z","shell.execute_reply.started":"2024-08-06T03:00:54.435622Z","shell.execute_reply":"2024-08-06T03:00:55.991774Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_and_evaluate(vit_model, train_loader, valid_loader, num_epochs=5)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T03:01:43.951611Z","iopub.execute_input":"2024-08-06T03:01:43.952326Z","iopub.status.idle":"2024-08-06T04:34:30.538909Z","shell.execute_reply.started":"2024-08-06T03:01:43.952296Z","shell.execute_reply":"2024-08-06T04:34:30.537612Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.017, Training Accuracy: 99.836%\nEpoch 1, Validation Loss: 0.022, Validation Accuracy: 99.925%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, valid_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 25\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"vit_model.eval()\n\n# 평가 함수 정의\ndef evaluate_model(model, valid_loader, device='cuda'):\n    model.to(device)\n    correct = 0\n    total = 0\n    valid_loss = 0.0\n    criterion = nn.CrossEntropyLoss()\n\n    with torch.no_grad():\n        for images, labels in valid_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            valid_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_valid_loss = valid_loss / len(valid_loader)\n    valid_accuracy = 100 * correct / total\n    print(f\"Validation Loss: {avg_valid_loss:.3f}, Validation Accuracy: {valid_accuracy:.3f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T04:43:41.571493Z","iopub.execute_input":"2024-08-06T04:43:41.571762Z","iopub.status.idle":"2024-08-06T04:43:41.912301Z","shell.execute_reply.started":"2024-08-06T04:43:41.571739Z","shell.execute_reply":"2024-08-06T04:43:41.911188Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 평가 함수 정의\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, valid_loader, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"evaluate_model(model, valid_loader)","metadata":{},"execution_count":null,"outputs":[]}]}